{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkC68Q6VbcTiGgdac7CL4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatgupta3121/Rajat_gupta_143/blob/main/AiChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8avsmcwgXU4S",
        "outputId": "010da5dc-4b8c-4a9b-f9e2-2c8d06808fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
            "  warnings.warn(\n",
            "2024-04-11 18:31:05.093 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Apr 11 21:19:26 2024\n",
        "\n",
        "@author: ocn\n",
        "\"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import json\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "import speech_recognition as sr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gtts import gTTS\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Load intents from JSON file\n",
        "with open(\"/content/intents.json\", \"r\") as file:\n",
        "    intents_data = json.load(file)\n",
        "\n",
        "# Preprocess intents data\n",
        "intents = intents_data[\"intents\"]\n",
        "\n",
        "# Tokenization and lemmatization\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token.lower()) for token in tokens]\n",
        "\n",
        "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return ' '.join(LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict))))\n",
        "\n",
        "# Initialize dictionaries to hold patterns and responses\n",
        "patterns_responses = {}\n",
        "\n",
        "# Extract patterns and responses from intents\n",
        "for intent in intents:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        # Join the pattern list into a single string\n",
        "        pattern_text = ' '.join(pattern)\n",
        "        patterns_responses[LemNormalize(pattern_text)] = intent[\"responses\"]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\",token_pattern=None)\n",
        "\n",
        "# Convert text data to TF-IDF vectors\n",
        "tfidf_matrix = TfidfVec.fit_transform(patterns_responses.keys())\n",
        "\n",
        "# Function to get response from the bot\n",
        "def get_response(user_response):\n",
        "    # Convert user input to TF-IDF vector\n",
        "    user_tfidf = TfidfVec.transform([user_response])\n",
        "\n",
        "    # Calculate cosine similarity between user input and patterns\n",
        "    similarity_scores = cosine_similarity(user_tfidf, tfidf_matrix)\n",
        "\n",
        "    # Get index of the most similar pattern\n",
        "    max_sim_index = similarity_scores.argmax()\n",
        "\n",
        "    # Get corresponding response for the most similar pattern\n",
        "    response = list(patterns_responses.values())[max_sim_index]\n",
        "\n",
        "    return random.choice(response)  # Randomly select a response\n",
        "\n",
        "# Function for speech to text conversion\n",
        "def speech_to_text():\n",
        "    r = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        st.write(\"Listening...\")\n",
        "        audio = r.listen(source)\n",
        "\n",
        "    try:\n",
        "        user_input = r.recognize_google(audio)\n",
        "        st.text_input(\"You:\", user_input)\n",
        "        return user_input\n",
        "    except sr.UnknownValueError:\n",
        "        st.write(\"Sorry, I couldn't understand what you said.\")\n",
        "    except sr.RequestError:\n",
        "        st.write(\"Sorry, my speech recognition service is unavailable at the moment.\")\n",
        "\n",
        "# Function for text to speech conversion\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(\"output.mp3\")\n",
        "    os.system(\"start output.mp3\")\n",
        "\n",
        "# Main function to interact with the bot\n",
        "def main():\n",
        "    st.title(\"Chat Bot\")\n",
        "    st.write(\"Hello! I am the Chat Bot. You can start chatting with me.\")\n",
        "\n",
        "\n",
        "\n",
        "    speech_input = st.button(\"Speak\")\n",
        "    if speech_input:\n",
        "        user_input = speech_to_text()\n",
        "        if user_input:\n",
        "            bot_response = get_response(user_input.lower())\n",
        "            st.text_area(\"Bot:\", value=bot_response, height=200)\n",
        "            text_to_speech(bot_response)\n",
        "\n",
        "    user_input = st.text_input(\"You:\", \"\")\n",
        "    if user_input:\n",
        "        bot_response = get_response(user_input.lower())\n",
        "        st.text_area(\"Bot:\", value=bot_response, height=200)\n",
        "        text_to_speech(bot_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "id": "F7d-VZ34X8uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "SqLzRmcJX89D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gtts"
      ],
      "metadata": {
        "id": "jRovNkqXX9Bd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}