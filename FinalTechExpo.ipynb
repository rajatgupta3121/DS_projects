{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2HTjX1SrbZPGFydmx/bXm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatgupta3121/Rajat_gupta_143/blob/main/FinalTechExpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "id": "ewvFVjnj215o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "-DKLn5QJ3BBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tNFBw6a3MVW",
        "outputId": "2c76f986-82b1-43ff-8fac-09e949615899"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.2.2)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import json\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "import speech_recognition as sr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gtts import gTTS\n",
        "import os"
      ],
      "metadata": {
        "id": "ZjxPbwnB2wGv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT2MJa8x34Ge",
        "outputId": "f8112632-45fa-42a7-c471-f25d1b51ce22"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load intents from JSON file\n",
        "with open(\"/content/intents.json\", \"r\") as file:\n",
        "    intents_data = json.load(file)"
      ],
      "metadata": {
        "id": "qlCI1ZbX3WYW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess intents data\n",
        "intents = intents_data[\"intents\"]"
      ],
      "metadata": {
        "id": "HdHkYH4y3iGH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and lemmatization\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "HCw7Qkxs3lIn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token.lower()) for token in tokens]"
      ],
      "metadata": {
        "id": "5TbAqQpP3lE8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)"
      ],
      "metadata": {
        "id": "YXqOvdRT3lBZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LemNormalize(text):\n",
        "    return ' '.join(LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict))))\n"
      ],
      "metadata": {
        "id": "9vO24l4-3k-V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to hold patterns and responses\n",
        "patterns_responses = {}\n"
      ],
      "metadata": {
        "id": "75XCY1BA3k7K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract patterns and responses from intents\n",
        "for intent in intents:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        # Join the pattern list into a single string\n",
        "        pattern_text = ' '.join(pattern)\n",
        "        patterns_responses[LemNormalize(pattern_text)] = intent[\"responses\"]"
      ],
      "metadata": {
        "id": "3y_cWIz53k3-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectorizer\n",
        "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\",token_pattern=None)\n",
        "\n",
        "# Convert text data to TF-IDF vectors\n",
        "tfidf_matrix = TfidfVec.fit_transform(patterns_responses.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr0J3Clz3kzm",
        "outputId": "b556d28b-c996-4d59-fedd-193ea4ad4ede"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get response from the bot\n",
        "def get_response(user_response):\n",
        "    # Convert user input to TF-IDF vector\n",
        "    user_tfidf = TfidfVec.transform([user_response])\n",
        "\n",
        "    # Calculate cosine similarity between user input and patterns\n",
        "    similarity_scores = cosine_similarity(user_tfidf, tfidf_matrix)\n",
        "\n",
        "    # Get index of the most similar pattern\n",
        "    max_sim_index = similarity_scores.argmax()\n",
        "\n",
        "    # Get corresponding response for the most similar pattern\n",
        "    response = list(patterns_responses.values())[max_sim_index]\n",
        "\n",
        "    return random.choice(response)  # Randomly select a response"
      ],
      "metadata": {
        "id": "g0CZuJWZ4Wic"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for speech to text conversion\n",
        "def speech_to_text():\n",
        "    r = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        st.write(\"Listening...\")\n",
        "        audio = r.listen(source)\n",
        "\n",
        "    try:\n",
        "        user_input = r.recognize_google(audio)\n",
        "        st.text_input(\"You:\", user_input)\n",
        "        return user_input\n",
        "    except sr.UnknownValueError:\n",
        "        st.write(\"Sorry, I couldn't understand what you said.\")\n",
        "    except sr.RequestError:\n",
        "        st.write(\"Sorry, my speech recognition service is unavailable at the moment.\")\n"
      ],
      "metadata": {
        "id": "7VmByIQd4YkH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for text to speech conversion\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(\"output.mp3\")\n",
        "    os.system(\"start output.mp3\")\n"
      ],
      "metadata": {
        "id": "miQQmv-M4cdT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to interact with the bot\n",
        "def main():\n",
        "    st.title(\"Chat Bot\")\n",
        "    st.write(\"Hello! I am the Chat Bot. You can start chatting with me.\")\n",
        "\n",
        "    # Display image of cute chatbot\n",
        "    st.image(\"/content/bot.png\", caption=\"Cute Chatbot\", use_column_width=True)\n",
        "\n",
        "    speech_input = st.button(\"Speak\")\n",
        "    if speech_input:\n",
        "        user_input = speech_to_text()\n",
        "        if user_input:\n",
        "            bot_response = get_response(user_input.lower())\n",
        "            st.text_area(\"Bot:\", value=bot_response, height=200)\n",
        "            text_to_speech(bot_response)\n",
        "\n",
        "    user_input = st.text_input(\"You:\", \"\")\n",
        "    if user_input:\n",
        "        bot_response = get_response(user_input.lower())\n",
        "        st.text_area(\"Bot:\", value=bot_response, height=200)\n",
        "        text_to_speech(bot_response)\n"
      ],
      "metadata": {
        "id": "2ucLXhVt4jFj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oIERAMm54mv8"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}